"""
Multilingual translator + romanizer (cleaned, commented)

Recommended installs for best results:
pip install langid deep-translator pykakasi pypinyin transliterate hangul-romanize

Notes:
- langid is used for detection (more stable than langdetect).
- deep-translator.GoogleTranslator does translation (source='auto').
- Some libraries (hangul-romanize, transliterate) are optional; script falls back gracefully.
- Urdu romanization uses an extended manual dictionary + character mapping for readable output.
"""
try:
    from indic_transliteration import sanscript
    from indic_transliteration.sanscript import transliterate
except Exception:
    transliterate = None
    sanscript = None

import langid
from deep_translator import GoogleTranslator

# optional libs (we try to import; if missing, we fall back gracefully)
try:
    import pykakasi  # Japanese -> romaji
except Exception:
    pykakasi = None

try:
    from pypinyin import lazy_pinyin  # Chinese -> pinyin
except Exception:
    lazy_pinyin = None

try:
    from transliterate import translit, get_available_language_codes  # Russian/Greek etc.
except Exception:
    translit = None
    get_available_language_codes = lambda: []

try:
    from hangul_romanize import Transliter
    from hangul_romanize.rule import academic
    korean_trans = Transliter(academic)
except Exception:
    korean_trans = None


import re

def clean_hinglish(itrans_text: str) -> str:
    """
    Convert academic-style romanization (Harvard-Kyoto/ITRANS) into
    smoother Hinglish (WhatsApp style).
    """
    text = itrans_text

    # Remove weird diacritics and over-markings
    text = text.replace(".N", "n").replace("M", "n").replace("~N", "n")
    text = text.replace(".a", "a").replace(".i", "i").replace(".u", "u")
    text = text.replace(".r", "r")

    replacements = {
        "maiM": "main",
        "tumase": "tumse",
        "pyAra": "pyaar",
        "karatA": "karta",
        "hU.n": "hoon",
        "hU.N": "hoon",
        "tumheM": "tumhe",
        "yAda": "yaad",
        "mujhe": "mujhe",
        "pUrA": "poora",
        "yakIna": "yakeen",
        "tuma": "tum",
        "aisA": "aisa",
        "karate": "karte"
    }
    for k, v in replacements.items():
        text = re.sub(r"\b" + re.escape(k) + r"\b", v, text)

    text = text.lower()
    text = re.sub(r"aa+", "aa", text)
    text = re.sub(r"ii+", "i", text)
    text = re.sub(r"uu+", "u", text)

    return text.strip()


# -------------------------
# LANGUAGE NAME -> CODE MAP
# Accepts both names and short codes
# -------------------------
LANG_MAP = {
    'english': 'en', 'en': 'en',
    'hindi': 'hi', 'hi': 'hi',
    'japanese': 'ja', 'ja': 'ja',
    'korean': 'ko', 'ko': 'ko',
    'spanish': 'es', 'es': 'es',
    'french': 'fr', 'fr': 'fr',
    'turkish': 'tr', 'tr': 'tr',
    'Dutch': 'nl', 'nl': 'nl',
    'german': 'de', 'de': 'de',
    'russian': 'ru', 'ru': 'ru',
    'italian': 'it', 'it': 'it',
    'chinese': 'zh-CN', 'zh': 'zh-CN', 'zh-cn': 'zh-CN', 'zh-tw': 'zh-TW',
    'latin': 'la', 'la': 'la',
    # 'cyrillic' is not a language; map to 'ru' as a practical proxy where needed
    'cyrillic': 'ru', 'sr': 'sr'
}

def normalize_target_lang(inp: str) -> str:
    """Return the translation target language code expected by GoogleTranslator."""
    if not inp:
        return 'en'
    key = inp.strip().lower()
    return LANG_MAP.get(key, key)

# -------------------------
# Urdu transliteration helpers (improved dictionary + char fallback)
# -------------------------
# Word-level replacements: common words/phrases to correct human-readable roman Urdu
URDU_WORD_MAP = {
    # Pronouns
    "Ù…ÛŒÚº": "main", "ØªÙ…": "tum", "Ø¢Ù¾": "aap", "ÛÙ…": "hum",
    "ÙˆÛ": "woh", "ÛŒÛ": "yeh", "ÙˆÛØ§Úº": "wahan", "ÛŒÛØ§Úº": "yahan",
    "Ù…ÛŒØ±Ø§": "mera", "Ù…ÛŒØ±ÛŒ": "meri", "Ù…ÛŒØ±Û’": "mere",
    "ÛÙ…Ø§Ø±Ø§": "hamara", "ÛÙ…Ø§Ø±ÛŒ": "hamari", "ÛÙ…Ø§Ø±Û’": "hamare",

    # Greetings & basics
    "Ø³Ù„Ø§Ù…": "salaam", "ÛÛŒÙ„Ùˆ": "hello", "ÛØ§Ø¦Û’": "hi",
    "Ú©ÛŒØ³Û’": "kaise", "ÛÙˆÚº": "hoon", "ÛÛ’": "hai", "ÛÛŒÚº": "hain",
    "ÛØ§Úº": "haan", "Ù†ÛÛŒÚº": "nahin", "Ù¹Ú¾ÛŒÚ©": "theek",

    # Common verbs
    "Ú©Ø±Ù†Ø§": "karna", "Ú©Ø±ØªØ§": "karta", "Ú©Ø±ØªÛŒ": "karti",
    "Ø¬Ø§Ù†Ø§": "jana", "Ø¬Ø§ØªØ§": "jata", "Ø¬Ø§ØªÛŒ": "jati",
    "Ú©Ú¾Ø§Ù†Ø§": "khana", "Ù¾ÛŒÙ†Ø§": "peena", "Ø³ÙˆÙ†Ø§": "sona",
    "Ø¯ÛŒÚ©Ú¾Ù†Ø§": "dekhna", "Ø¢Ù†Ø§": "aana", "Ø¯ÛŒÙ†Ø§": "dena", "Ù„ÛŒÙ†Ø§": "lena",

    # Time & daily
    "Ø¢Ø¬": "aaj", "Ú©Ù„": "kal", "Ø§Ø¨": "ab", "Ù¾Ú¾Ø±": "phir",
    "ØµØ¨Ø­": "subah", "Ø´Ø§Ù…": "shaam", "Ø±Ø§Øª": "raat", "Ø¯Ù†": "din",
    "ÛÙØªÛ": "hafta", "Ù…ÛÛŒÙ†Û": "mahina", "Ø³Ø§Ù„": "saal",

    # Feelings
    "Ù¾ÛŒØ§Ø±": "pyaar", "Ù…Ø­Ø¨Øª": "mohabbat", "Ø®ÙˆØ´ÛŒ": "khushi",
    "ØºÙ…": "gham", "Ø²Ù†Ø¯Ú¯ÛŒ": "zindagi", "Ø¯Ù„": "dil",
    "Ø¯Ù†ÛŒØ§": "duniya", "Ø§Ù„Ù„Û": "Allah", "Ø§Ù†Ø³Ø§Ù†": "insaan",

    # Numbers (1â€“10)
    "Ø§ÛŒÚ©": "ek", "Ø¯Ùˆ": "do", "ØªÛŒÙ†": "teen", "Ú†Ø§Ø±": "chaar", "Ù¾Ø§Ù†Ú†": "paanch",
    "Ú†Ú¾": "chhay", "Ø³Ø§Øª": "saat", "Ø¢Ù¹Ú¾": "aath", "Ù†Ùˆ": "nau", "Ø¯Ø³": "das",

    # OTHERS
    "Ù¹Ú©Ù†Ø§Ù„ÙˆØ¬ÛŒ" : "Technology",
    "Ù¹ÙˆÙ„" : "tool",
    "Ø¨Ø§Ú©Ø³" :"box", "Ø®Ù„Ø§" : "space",
    
}

# Character-level mapping fallback (better than nothing)
URDU_CHAR_MAP = {
    'Ø§': 'a', 'Ø¢': 'aa',
    'Ø¨': 'b', 'Ù¾': 'p',
    'Øª': 't', 'Ù¹': 't',
    'Ø«': 's',
    'Ø¬': 'j', 'Ú†': 'ch',
    'Ø­': 'h', 'Ø®': 'kh',
    'Ø¯': 'd', 'Úˆ': 'd',
    'Ø°': 'z',
    'Ø±': 'r', 'Ú‘': 'r',
    'Ø²': 'z', 'Ú˜': 'zh',
    'Ø³': 's', 'Ø´': 'sh',
    'Øµ': 's', 'Ø¶': 'z',
    'Ø·': 't', 'Ø¸': 'z',
    'Ø¹': "'", 'Øº': 'gh',
    'Ù': 'f',
    'Ù‚': 'q',
    'Ú©': 'k', 'Ú¯': 'g',
    'Ù„': 'l',
    'Ù…': 'm',
    'Ù†': 'n', 'Úº': 'n',
    'Ùˆ': 'o', 'Ø¤': 'o',
    'Û': 'h', 'Ú¾': 'h',
    'ÛŒ': 'y', 'Û’': 'e', 'Ø¦': 'i',
    'Ø¡': "'", 'Ù“': '', 'Ù”': '',
}


def romanize_urdu_text(text: str) -> str:
    """
    Smart-ish romanization for Urdu:
    - Try word replacements first (gives natural results for common words)
    - Then fall back to character mapping for remaining characters
    """
    # Normalize spacing (split by whitespace)
    words = text.split()
    out_words = []
    for w in words:
        # exact-word map
        if w in URDU_WORD_MAP:
            out_words.append(URDU_WORD_MAP[w])
            continue
        # punctuation-aware: strip common punctuation, map, then reattach
        prefix = ''
        suffix = ''
        core = w
        # preserve punctuation at start/end
        while core and not core[0].isalnum():
            prefix += core[0]; core = core[1:]
        while core and not core[-1].isalnum():
            suffix = core[-1] + suffix; core = core[:-1]
        if not core:
            out_words.append(prefix + suffix)
            continue
        # if whole core in dict
        if core in URDU_WORD_MAP:
            out_words.append(prefix + URDU_WORD_MAP[core] + suffix)
            continue
        # else character-by-character
        roman = []
        for ch in core:
            roman.append(URDU_CHAR_MAP.get(ch, ch))
        out_words.append(prefix + ''.join(roman) + suffix)
    return ' '.join(out_words)

# -------------------------
# Arabic & Persian simple transliteration map (readable approximate)
# These are crude but produce human-readable ascii text (not perfect IPA)
# -------------------------
ARABIC_CHAR_MAP = {
    'Ø§': 'a', 'Ø¨': 'b', 'Øª': 't', 'Ø«': 'th', 'Ø¬': 'j', 'Ø­': 'h', 'Ø®': 'kh',
    'Ø¯': 'd', 'Ø°': 'dh', 'Ø±': 'r', 'Ø²': 'z', 'Ø³': 's', 'Ø´': 'sh', 'Øµ': 's',
    'Ø¶': 'd', 'Ø·': 't', 'Ø¸': 'z', 'Ø¹': "'", 'Øº': 'gh', 'Ù': 'f', 'Ù‚': 'q',
    'Ú©': 'k', 'Ú¯': 'g', 'Ù„': 'l', 'Ù…': 'm', 'Ù†': 'n', 'Ùˆ': 'u', 'Ù‡': 'h',
    'ÛŒ': 'y', 'Ø¡': "'", 'Ø£': 'a', 'Ø¥': 'i', 'Ø¤': 'u', 'Ø¦': 'i', 'Ù‰': 'a',
    'Ø¢': 'aa', 'Ø©': 'a', 'Ú†':'che', 'ÙŠ' : 'i', 'Ùƒ' : 'ek', 'Ù¾' :'p',
}

def romanize_arabic_like(text: str) -> str:
    """Rough transliteration for Arabic/Persian script to readable ASCII."""
    out = []
    for ch in text:
        out.append(ARABIC_CHAR_MAP.get(ch, ch))
    return ''.join(out)

# Hindi Romanization

HINDI_WORD_MAP = {
    # Pronouns
    "à¤®à¥ˆà¤‚": "main", "à¤¤à¥à¤®": "tum", "à¤†à¤ª": "aap", "à¤¹à¤®": "hum",
    "à¤µà¤¹": "vah", "à¤¯à¥‡": "ye", "à¤¯à¤¹": "yeh", "à¤µà¥‡": "ve",
    "à¤®à¥‡à¤°à¤¾": "mera", "à¤®à¥‡à¤°à¥€": "meri", "à¤®à¥‡à¤°à¥‡": "mere",
    "à¤¹à¤®à¤¾à¤°à¤¾": "hamara", "à¤¹à¤®à¤¾à¤°à¥€": "hamari", "à¤¹à¤®à¤¾à¤°à¥‡": "hamare",

    # Greetings & basics
    "à¤¨à¤®à¤¸à¥à¤¤à¥‡": "namaste", "à¤¶à¥à¤­": "shubh", "à¤ªà¥à¤°à¤£à¤¾à¤®": "pranam",
    "à¤•à¥ˆà¤¸à¥‡": "kaise", "à¤¹à¥‹": "ho", "à¤¹à¥‚à¤": "hoon", "à¤¹à¥ˆà¤‚": "hain", "à¤¹à¥ˆ": "hai",
    "à¤¹à¤¾à¤": "haan", "à¤¨à¤¹à¥€à¤‚": "nahin", "à¤ à¥€à¤•": "theek",

    # Common verbs
    "à¤•à¤°à¤¨à¤¾": "karna", "à¤•à¤°à¤¤à¥‡": "karte", "à¤•à¤°à¤¤à¥€": "kartii", "à¤•à¤°": "kar",
    "à¤œà¤¾à¤¨à¤¾": "jana", "à¤œà¤¾à¤¤à¤¾": "jata", "à¤œà¤¾à¤¤à¥€": "jati",
    "à¤–à¤¾à¤¨à¤¾": "khana", "à¤ªà¥€à¤¨à¤¾": "peena", "à¤¸à¥‹à¤¨à¤¾": "sona",
    "à¤¦à¥‡à¤–à¤¨à¤¾": "dekhna", "à¤†à¤¨à¤¾": "aana", "à¤¦à¥‡à¤¨à¤¾": "dena", "à¤²à¥‡à¤¨à¤¾": "lena",

    # Time & daily
    "à¤†à¤œ": "aaj", "à¤•à¤²": "kal", "à¤…à¤¬": "ab", "à¤«à¤¿à¤°": "phir",
    "à¤¸à¥à¤¬à¤¹": "subah", "à¤¶à¤¾à¤®": "shaam", "à¤°à¤¾à¤¤": "raat", "à¤¦à¤¿à¤¨": "din",
    "à¤¸à¤ªà¥à¤¤à¤¾à¤¹": "saptah", "à¤®à¤¹à¥€à¤¨à¤¾": "mahina", "à¤¸à¤¾à¤²": "saal",

    # Feelings
    "à¤ªà¥à¤¯à¤¾à¤°": "pyaar", "à¤ªà¥à¤°à¥‡à¤®": "prem", "à¤–à¥à¤¶à¥€": "khushi",
    "à¤¦à¥à¤–": "dukh", "à¤œà¥€à¤µà¤¨": "jeevan", "à¤¦à¤¿à¤²": "dil",
    "à¤¦à¥à¤¨à¤¿à¤¯à¤¾": "duniya", "à¤­à¤—à¤µà¤¾à¤¨": "bhagwan",

    # Numbers (1â€“10)
    "à¤à¤•": "ek", "à¤¦à¥‹": "do", "à¤¤à¥€à¤¨": "teen", "à¤šà¤¾à¤°": "chaar", "à¤ªà¤¾à¤‚à¤š": "paanch",
    "à¤›à¤¹": "chhah", "à¤¸à¤¾à¤¤": "saat", "à¤†à¤ ": "aath", "à¤¨à¥Œ": "nau", "à¤¦à¤¸": "das",
}

HINDI_CHAR_MAP = {
    # Vowels
    'à¤…': 'a', 'à¤†': 'aa', 'à¤‡': 'i', 'à¤ˆ': 'ee',
    'à¤‰': 'u', 'à¤Š': 'oo', 'à¤‹': 'ri', 'à¥ ': 'ri',
    'à¤': 'e', 'à¤': 'ai', 'à¤“': 'o', 'à¤”': 'au',
    'à¥': 'om',

    # Consonants
    'à¤•': 'k', 'à¤–': 'kh', 'à¤—': 'g', 'à¤˜': 'gh', 'à¤™': 'n',
    'à¤š': 'ch', 'à¤›': 'chh', 'à¤œ': 'j', 'à¤': 'jh', 'à¤ž': 'n',
    'à¤Ÿ': 't', 'à¤ ': 'th', 'à¤¡': 'd', 'à¤¢': 'dh', 'à¤£': 'n',
    'à¤¤': 't', 'à¤¥': 'th', 'à¤¦': 'd', 'à¤§': 'dh', 'à¤¨': 'n',
    'à¤ª': 'p', 'à¤«': 'ph', 'à¤¬': 'b', 'à¤­': 'bh', 'à¤®': 'm',
    'à¤¯': 'y', 'à¤°': 'r', 'à¤²': 'l', 'à¤µ': 'v',

    # Sibilants + aspirates
    'à¤¶': 'sh', 'à¤·': 'sh', 'à¤¸': 's', 'à¤¹': 'h',

    # Conjuncts / special
    'à¤•à¥à¤·': 'ksh', 'à¤¤à¥à¤°': 'tr', 'à¤œà¥à¤ž': 'gy',

    # Nukta letters (borrowed sounds)
    'à¤•à¤¼': 'q', 'à¤–à¤¼': 'kh', 'à¤—à¤¼': 'gh', 'à¤œà¤¼': 'z', 'à¤¡à¤¼': 'r', 'à¤¢à¤¼': 'rh',
    'à¤«à¤¼': 'f', 'à¤à¤¼': 'zh',

    # Signs / diacritics
    'à¤‚': 'n', 'à¤': 'n', 'à¤ƒ': 'h', 'à¤¼': '', 'à¥': '',
    'à¥Œ': 'au', 'à¥ˆ': 'ai', 'à¥‰': 'o', 'à¥†': 'e', 'à¥Š': 'o',
}

def romanize_hindi_text(text: str) -> str:
    """
    Romanization for Hindi:
    - Check HINDI_WORD_MAP first
    - Else use character-by-character mapping
    """
    words = text.split()
    out_words = []
    for w in words:
        if w in HINDI_WORD_MAP:
            out_words.append(HINDI_WORD_MAP[w])
            continue
        roman = []
        for ch in w:
            roman.append(HINDI_CHAR_MAP.get(ch, ch))
        out_words.append(''.join(roman))
    return ' '.join(out_words)
# -------------------------
# Rome/romanize dispatcher
# -------------------------
def romanize_text(text: str, lang_code: str) -> str:
    """
    Return a romanized/transliterated version of text according to lang_code.
    Falls back to reasonable defaults if optional libs aren't installed.
    """
    if not text:
        return text

    # Normalize code forms
    lc = (lang_code or '').lower()

    # Japanese -> using pykakasi if available
    # Japanese -> romaji using new pykakasi API
    if lc.startswith('ja') or lc == 'japanese':
        if pykakasi:
           kakasi = pykakasi.kakasi()
           result = kakasi.convert(text)
           return " ".join([item['hepburn'] for item in result])
        else:
           return text
  # no pykakasi: return original

    # Chinese -> pinyin
    if lc.startswith('zh'):
        if lazy_pinyin:
            try:
                return ' '.join(lazy_pinyin(text))
            except Exception:
                return text
        return text

    # Korean -> hangul-romanize if available
    if lc.startswith('ko') or lc == 'korean':
        if korean_trans:
            try:
                return korean_trans.translit(text)
            except Exception:
                return text
        return text

    # Urdu -> our improved dictionary-based romanizer
    if lc == 'ur' or lc == 'urdu':
        return romanize_urdu_text(text)

    # Hindi -> try transliterate library (Devanagari -> Latin)
    if lc == 'hi' or lc == 'hindi':
       if transliterate and sanscript:
          try:
              raw = transliterate(text, sanscript.DEVANAGARI, sanscript.ITRANS)
              return clean_hinglish(raw)  # ðŸ‘ˆ scrub output into Hinglish
          except Exception:
              return romanize_hindi_text(text)  # fallback
       else:
          return romanize_hindi_text(text)
  
    # Arabic / Persian -> our simple char-map fallback
    if lc in ('ar', 'arabic', 'fa', 'persian', 'farsi'):
        return romanize_arabic_like(text)

    # Russian / Serbian / other cyrillic -> transliterate if available
    if lc in ('ru', 'sr', 'cyrillic', 'russian'):
        if translit:
            try:
                return translit(text, 'ru', reversed=True)
            except Exception:
                return text
        return text

    # Greek
    if lc in ('el', 'greek'):
        if translit:
            try:
                return translit(text, 'el', reversed=True)
            except Exception:
                return text
        return text

    # Latin-based languages: return as-is (already readable)
    if lc in ('en', 'fr', 'es', 'it', 'la', 'de', 'pt', 'spanish', 'french', 'italian', 'latin'):
        return text

    # Default fallback: return original
    return text

# -------------------------
# Language detection with confidence and ASCII-short-text fallback
# -------------------------
def detect_language_safely(text: str) -> str:
    """
    Use langid to classify language. If confidence is low and text is ASCII,
    assume English. This cuts down on strange short-text misclassifications.
    """
    if not text or not text.strip():
        return 'unknown'
    lang, conf = langid.classify(text)
    # if confidence low and text is mostly ASCII letters/punct, prefer English
    if conf < 0.90:
        # if all characters are ASCII and many English-looking words, assume en
        num_ascii = sum(1 for ch in text if ord(ch) < 128)
        if num_ascii / max(1, len(text)) > 0.9:
            # simple heuristic: presence of common English words
            lowers = text.lower()
            english_signals = ['the ', ' and ', ' how ', ' you', ' is ', ' are ', ' hello', ' hi ']
            if any(sig in lowers for sig in english_signals):
                return 'en'
    return lang

# -------------------------
# Main translate + romanize
# -------------------------
def translate_and_romanize(text: str, target_lang_code: str):
    """
    - Detect source language (safe)
    - Translate using GoogleTranslator (deep-translator)
    - Romanize translated text if needed according to target_lang_code
    Returns (src_lang, translated_text, romanized_text)
    """
    src = detect_language_safely(text)
    # Use normalized target lang (e.g., 'urdu' -> 'ur')
    tgt = normalize_target_lang(target_lang_code)

    try:
        translated = GoogleTranslator(source='auto', target=tgt).translate(text)
    except Exception as e:
        translated = f"(translation error: {e})"

    roman = romanize_text(translated, tgt)
    return src, translated, roman

# -------------------------
# CLI loop
# -------------------------
def prompt_loop():
    print("Multilingual translator â€” supports: urdu, japanese, persian, chinese, english, russian, latin, french, spanish, italian, korean, arabic, hindi, cyrillic, greek.")
    print("Type text, choose a target language (name or code). Libraries optional; script will try fallbacks.")
    while True:
        text = input("\nEnter text: ").strip()
        if not text:
            print("No text entered. Try again.")
            continue

        target_input = input("Translate into which language (name or code, e.g. 'ur' or 'Russian'): ").strip()
        if not target_input:
            target_input = 'en'
        src_lang, translated, roman = translate_and_romanize(text, target_input)

        print(f"\nDetected Language (heuristic): {src_lang}")
        print(f"Translated ({normalize_target_lang(target_input)}): {translated}")
        print(f"Romanized: {roman}\n")

        again = input("Do you want to translate more? (yes/no): ").strip().lower()
        if again not in ('yes', 'y'):
            print("Exiting translator. Bye.")
            break


from flask import Flask, send_from_directory, request, jsonify
from flask_cors import CORS
import os
# React build path
REACT_BUILD_DIR = os.path.join(os.path.dirname(__file__), "../translator-ui/build")

app = Flask(__name__, static_folder=REACT_BUILD_DIR, static_url_path="/")
CORS(app)

# Serve React frontend
@app.route("/")
def serve_react():
    return send_from_directory(app.static_folder, "index.html")

# API endpoint
@app.route("/api/translate", methods=["POST"])
def translate_api():
    data = request.get_json()
    text = data.get("text", "")
    target = data.get("target", "en")

    if not text.strip():
        return jsonify({
            "source_lang": "-",
            "translated": "",
            "romanized": ""
        })

    src_lang, translated, romanized = translate_and_romanize(text, target)

    return jsonify({
        "source_lang": src_lang,
        "translated": translated,
        "romanized": romanized
    })

# For React Router â€” serve index.html for all other paths
@app.errorhandler(404)
def not_found(e):
    return send_from_directory(app.static_folder, "index.html")

if __name__ == "__main__":
    app.run(debug=True)
